import type { HttpResponse } from '@sap-cloud-sdk/http-client';
import type {
  CompletionPostResponse,
  TokenUsage,
  ChatMessage,
  ChatMessages,
  AssistantChatMessage,
  MessageToolCalls
} from './client/api/schema/index.js';

/**
 * Representation of an orchestration response.
 */
export class OrchestrationResponse {
  /**
   * The completion post response.
   */
  public readonly data: CompletionPostResponse;
  constructor(public readonly rawResponse: HttpResponse) {
    this.data = rawResponse.data;
  }

  /**
   * Usage of tokens in the response.
   * @returns Token usage.
   */
  getTokenUsage(): TokenUsage {
    return this.data.orchestration_result.usage!;
  }
  /**
   * Reason for stopping the completion.
   * @param choiceIndex - The index of the choice to parse.
   * @returns The finish reason.
   */
  getFinishReason(choiceIndex = 0): string | undefined {
    return this.findChoiceByIndex(choiceIndex)?.finish_reason;
  }

  /**
   * Parses the orchestration response and returns the content.
   * If the response was filtered, an error is thrown.
   * @param choiceIndex - The index of the choice to parse.
   * @returns The message content.
   */
  getContent(choiceIndex = 0): string | undefined {
    const choice = this.findChoiceByIndex(choiceIndex);
    if (
      choice?.message?.content === '' &&
      choice?.finish_reason === 'content_filter'
    ) {
      throw new Error(
        'Content generated by the LLM was filtered by the output filter. Please try again with a different prompt or filter configuration.'
      );
    }
    return choice?.message?.content;
  }

  /**
   * Parses the orchestration response and returns the tool calls generated by the model.
   * @param choiceIndex - The index of the choice to parse.
   * @returns The message tool calls.
   */
  getToolCalls(choiceIndex = 0): MessageToolCalls | undefined {
    const choice = this.findChoiceByIndex(choiceIndex);
    return choice?.message?.tool_calls;
  }

  /**
   * Parses the orchestration response and returns the refusal message generated by the model.
   * @param choiceIndex - The index of the choice to parse.
   * @returns The refusal string.
   */
  getRefusal(choiceIndex = 0): string | undefined {
    const choice = this.findChoiceByIndex(choiceIndex);
    return choice?.message?.refusal;
  }

  /**
   * Messages that can be used for subsequent prompts as message history.
   * @param choiceIndex - The index of the choice to parse.
   * @returns A list of all messages.
   */
  getAllMessages(choiceIndex = 0): ChatMessages {
    const messages: ChatMessage[] = this.data.module_results.templating ?? [];
    const content = this.findChoiceByIndex(choiceIndex)?.message;
    return content ? [...messages, content] : messages;
  }

  /**
   * Gets the assistant message from the response.
   * @param choiceIndex - The index of the choice to use (default is 0).
   * @returns The assistant message.
   */
  getAssistantMessage(choiceIndex = 0): AssistantChatMessage | undefined {
    return this.findChoiceByIndex(choiceIndex)?.message;
  }

  private getChoices() {
    return this.data.orchestration_result.choices;
  }

  private findChoiceByIndex(index: number) {
    // TODO: replace cast with LLMChoice[] after the bug in orchestration, where
    // 'role' in ResponseChatMessage is optional when it should be mandatory, is fixed.
    // https://github.com/SAP/ai-sdk-js-backlog/issues/306
    return (this.getChoices() as any).find(
      (c: { index: number }) => c.index === index
    );
  }
}
